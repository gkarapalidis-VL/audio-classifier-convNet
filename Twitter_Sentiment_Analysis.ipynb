{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkarapalidis-VL/audio-classifier-convNet/blob/master/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TqcigocSPf-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Twitter Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "OK9us1CbPjvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importa data\n",
        "####Connect to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "WK-sHjFoOEyQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJPup5roPprz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_F2tIDYP4LA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Get CSV files from Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "nlVFQRSBP3Va",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test Data\n",
        "\n",
        "testData = 'https://drive.google.com/open?id=14_vpGneHXHf9MUXtGYe3NGiEs4GriBZk'\n",
        "fluff, testDataID = testData.split('=')\n",
        "print (testDataID) # Verify that you have everything after '='\n",
        "\n",
        "\n",
        "#Training Data\n",
        "\n",
        "trainData = 'https://drive.google.com/open?id=1fl608tMF7rxLJ7fItGQDBSXck2QJq03p'\n",
        "fluff, trainDataID = trainData.split('=')\n",
        "print (trainDataID) # Verify that you have everything after '='"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCONSn5tQP5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download Data from Google Drive\n",
        "\n",
        "downloadTestData = drive.CreateFile({'id':trainDataID}) \n",
        "downloadTestData.GetContentFile('testdata.manual.2009.06.14.csv') \n",
        "\n",
        "downloadTrainData = drive.CreateFile({'id':trainDataID}) \n",
        "downloadTrainData.GetContentFile('training.1600000.processed.noemoticon.csv') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MoCHWZwbV_Hs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check if the data has been uploaded\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLkTr4O1Q609",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Specify column names\n",
        "\n",
        "#0 — the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "#1 — the id of the tweet (2087)\n",
        "#2 — the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "#3 — the query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "#4 — the user that tweeted (robotickilldozr)\n",
        "#5 — the text of the tweet (Lyx is cool)\n",
        "\n",
        "\n",
        "cols = ['sentiment','id','date','query_string','user','text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmZFR4AWRJsB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load training data to a dataframe\n",
        "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None, names=cols, encoding=\"ISO-8859–1\")\n",
        "\n",
        "#display head (first 5 lines)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dFCUMAIwRUbY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#count sentiment values\n",
        "df.sentiment.value_counts()\n",
        "\n",
        "#total 1.6M entries 800K positive 800K negative\n",
        "#positive value 4, negative value 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggW5TUv_RyId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#drop the columns we don't need\n",
        "#“id” column is unique ID for each tweet\n",
        "#“date” column is for date info for the tweet\n",
        "#“query_string” column indicates whether the tweet has been collected with any particular query key word, but for this column, 100% of the entries are with value “NO_QUERY”\n",
        "#“user” column is the twitter handle name for the user who tweeted\n",
        "\n",
        "df.drop(['id','date','query_string','user'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJST8S2fR_te",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#display 10 first rows with negative sentiment\n",
        "df[df.sentiment == 0].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmgAkBRESHPZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#display 10 first rows with positive sentiment\n",
        "df[df.sentiment == 4].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eTkw43jfSQgI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By looking at some entries for each class, it seems like that all the negative class is from 0~799999th index, and the positive class entries start from 800000 to the end of the dataset."
      ]
    },
    {
      "metadata": {
        "id": "AQIU6a9wSS5I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ]
    },
    {
      "metadata": {
        "id": "WA_zrAlMST4w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Check the length of the string in text column in each entry\n",
        "df['pre_clean_len'] = [len(t) for t in df.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kzJkf2UShWw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data disctionary, first draft"
      ]
    },
    {
      "metadata": {
        "id": "u7mERBH_SkSP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "data_dict = {\n",
        "    'sentiment':{\n",
        "        'type':df.sentiment.dtype,\n",
        "        'description':'sentiment class - 0:negative, 1:positive'\n",
        "    },\n",
        "    'text':{\n",
        "        'type':df.text.dtype,\n",
        "        'description':'tweet text'\n",
        "    },\n",
        "    'pre_clean_len':{\n",
        "        'type':df.pre_clean_len.dtype,\n",
        "        'description':'Length of the tweet before cleaning'\n",
        "    },\n",
        "    'dataset_shape':df.shape\n",
        "}\n",
        "pprint(data_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yod5GRC3TDKG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#plot pre_clean_len with box plot, so that we can see the overall distribution of length of strings in each entry.\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "plt.boxplot(df.pre_clean_len)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TcYBDNvRW1Mr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[df.pre_clean_len > 140].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ZSrl1QSjC53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preparation 1: HTML decoding"
      ]
    },
    {
      "metadata": {
        "id": "rDiDiEW-XlEz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#display a text from a row to see the format\n",
        "df.text[279]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOVuyHHwjRMh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It looks like HTML encoding has not been converted to text, and ended up in text field as ‘&amp’,’&quot’,etc. Decoding HTML to general text will be my first step of data preparation.\n",
        "\n",
        "We will use BeautifulSoup for this."
      ]
    },
    {
      "metadata": {
        "id": "kU8evBpzjVd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "example1 = BeautifulSoup(df.text[279], 'lxml')\n",
        "print (example1.get_text())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PBCO8ZKkZzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preparation 2: ‘@’mention"
      ]
    },
    {
      "metadata": {
        "id": "giroEYyvkedi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The second part of the preparation is dealing with @mention.\n",
        "Even though @mention carries a certain information (which another user that the tweet mentioned), this information doesn’t add value to build sentiment analysis model."
      ]
    },
    {
      "metadata": {
        "id": "_xCny7xRjarc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.text[343]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGTITnJkkgFL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "re.sub(r'@[A-Za-z0-9]+','',df.text[343])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3TmCAJuknvG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preparation 3: URL links"
      ]
    },
    {
      "metadata": {
        "id": "L8QRkNzjkj2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.text[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSk-pHgkko3z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "re.sub('https?://[A-Za-z0-9./]+','',df.text[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TdiMh3qlkwHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preparation 4: UTF-8 BOM (Byte Order Mark)\n"
      ]
    },
    {
      "metadata": {
        "id": "IzdrwjoTkuGk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.text[226]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrMpbi9SkxE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testing = df.text[226]\n",
        "testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ft9BF9Krk5iM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testing.replace(u\"ï¿½\", \"?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6z1utdk0mkEm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preparation 5: hashtag / numbers"
      ]
    },
    {
      "metadata": {
        "id": "MuU5AObqmeQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.text[175]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FE6qm7YZmmrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#remove \"#\" form the text\n",
        "re.sub(\"[^a-zA-Z]\", \" \", df.text[175])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8_BBERUmqBR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nruDiEjLmu7O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining data cleaning function\n",
        "\n",
        "With above five data cleaning tasks, we will first define data cleaning function, and then will be applied to the whole dataset. \n",
        "\n",
        "The order of the cleaning is\n",
        "\n",
        "1. Souping\n",
        "2. BOM removing\n",
        "3. url address(‘http:’pattern), twitter ID removing\n",
        "4. url address(‘www.'pattern) removing\n",
        "5. lower-case\n",
        "6. negation handling\n",
        "7. removing numbers and special characters\n",
        "8. tokenizing and joining\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V4P1pmmImvwJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()\n",
        "\n",
        "pat1 = r'@[A-Za-z0-9_]+'\n",
        "pat2 = r'https?://[^ ]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "www_pat = r'www.[^ ]+'\n",
        "\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"}\n",
        "\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    try:\n",
        "        bom_removed = souped.replace(u\"ï¿½\", \"?\")\n",
        "    except:\n",
        "        bom_removed = souped\n",
        "    stripped = re.sub(combined_pat, '', bom_removed)\n",
        "    stripped = re.sub(www_pat, '', stripped)\n",
        "    lower_case = stripped.lower()\n",
        "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
        "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
        "    # I will tokenize and join together to remove unneccessary white spaces\n",
        "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
        "    return (\" \".join(words)).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MdaBGQa_w8_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testing = df.text[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iCGavKznw98R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_result = []\n",
        "for t in testing:\n",
        "    test_result.append(tweet_cleaner(t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQE9lArRxCkV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuAK94g0m-B5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xrange=range\n",
        "nums = [0,400000,800000,1200000,1600000]\n",
        "print (\"Cleaning and parsing the text...\\n\")\n",
        "clean_tweet_texts = []\n",
        "for i in xrange(nums[0],nums[1]):\n",
        "    if( (i+1)%10000 == 0 ):\n",
        "        print (\"Text %d of %d has been processed\" % ( i+1, nums[1] ))                                                                    \n",
        "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uXPGrCV2nD1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(clean_tweet_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4SeFz-Yp32G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print (\"Cleaning and parsing the text...\\n\")\n",
        "for i in range(nums[1],nums[2]):\n",
        "    if( (i+1)%10000 == 0 ):\n",
        "        print (\"Text %d of %d has been processed\" % ( i+1, nums[2] ) )                                                                  \n",
        "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ucmNYWzkp9O-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(clean_tweet_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VD7gjoS1qs5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print (\"Cleaning and parsing the text...\\n\")\n",
        "for i in range(nums[2],nums[3]):\n",
        "    if( (i+1)%10000 == 0 ):\n",
        "        print (\"Text %d of %d has been processed\" % ( i+1, nums[3] ) )                                                                  \n",
        "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q5p56GQwqves",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(clean_tweet_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "knME--D_q8ZW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print (\"Cleaning and parsing the text...\\n\")\n",
        "for i in range(nums[3],nums[4]):\n",
        "    if( (i+1)%10000 == 0 ):\n",
        "        print (\"Text %d of %d has been processed\" % ( i+1, nums[4] ) )                                                                  \n",
        "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQsoO4akr32T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Saving cleaned data as csv"
      ]
    },
    {
      "metadata": {
        "id": "PmHmdfCbr3TJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
        "clean_df['target'] = df.sentiment\n",
        "clean_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2gniO_4r6wb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_df.to_csv('clean_tweet.csv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJYnpewfr_QH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csv = 'clean_tweet.csv'\n",
        "my_df = pd.read_csv(csv,index_col=0)\n",
        "my_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGUdA_t_sK33",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download saved file"
      ]
    },
    {
      "metadata": {
        "id": "kXSg6CQAsKoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('clean_tweet.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVXEXvVLz1N5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwMpfCBH0Vrn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load clean dataset into a new dataframe"
      ]
    },
    {
      "metadata": {
        "id": "91CctbiJ0ZyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csv = 'clean_tweet.csv'\n",
        "my_df = pd.read_csv(csv,index_col=0)\n",
        "my_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFLe9C670cFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rttktWHh0mSl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "there are some null entries in the data, let’s investigate further."
      ]
    },
    {
      "metadata": {
        "id": "a9KEL_Wl0gSp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#return the first 5 null text entries\n",
        "my_df[my_df.isnull().any(axis=1)].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nF-E3XV_0oLR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#return the sum of all null text entries\n",
        "np.sum(my_df.isnull().any(axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pG8Rs5Fp0xR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#return which columns have null values\n",
        "my_df.isnull().any(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrfxAeQZ05-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It seems like 3,959 entries have null entries for text column. This is strange, because the original dataset had no null entries, thus if there are any null entries in the cleaned dataset, it must have happened during the cleaning process."
      ]
    },
    {
      "metadata": {
        "id": "a4sSdFKH01fp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#return null entries from the pre-cleaned dataset\n",
        "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",header=None,encoding=\"ISO-8859–1\")\n",
        "df.iloc[my_df[my_df.isnull().any(axis=1)].index,:].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NcwRoWw1US4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By looking these entries in the original data, it seems like only text information they had was either twitter ID or url address. These are info to discard for the sentiment analysis, so we will drop these null rows, and update the data frame."
      ]
    },
    {
      "metadata": {
        "id": "foyqQ8ex0-UR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_df.dropna(inplace=True)\n",
        "my_df.reset_index(drop=True,inplace=True)\n",
        "my_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4UtFEIH1hmh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "metadata": {
        "id": "yQRI1Ynf1m2G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word Cloud"
      ]
    },
    {
      "metadata": {
        "id": "bt7BtZn_1dER",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create word cloud for tweets with negative sentiment\n",
        "neg_tweets = my_df[my_df.target == 0]\n",
        "neg_string = []\n",
        "\n",
        "for t in neg_tweets.text:\n",
        "    neg_string.append(t)\n",
        "neg_string = pd.Series(neg_string).str.cat(sep=' ')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9amhoSoI2E1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some of big words can be interpreted quite neutral, such as “today”,”now”,etc. We can see some of the words in smaller size make sense to be in negative tweets, such as “damn”,”ugh”,”miss”,”bad”, etc. But there is “love” in rather big size, so let's see what is happening."
      ]
    },
    {
      "metadata": {
        "id": "ua8EILGD1tcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#return tweets that include \"love\"\n",
        "for t in neg_tweets.text[:200]:\n",
        "    if 'love' in t:\n",
        "        print (t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j795K9nl2n53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Even though the tweets contain the word “love”, in these cases it is negative sentiment, because the tweet has mixed emotions like “love” but “miss”. Or sometimes used in a sarcastic way."
      ]
    },
    {
      "metadata": {
        "id": "n3MORSPJ2fdx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create word cloud for tweets with positive sentiment\n",
        "pos_tweets = my_df[my_df.target == 4]\n",
        "pos_string = []\n",
        "\n",
        "for t in pos_tweets.text:\n",
        "    pos_string.append(t)\n",
        "pos_string = pd.Series(pos_string).str.cat(sep=' ')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(pos_string)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mx0-DhPa5y7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#remap positive value 4 to 1\n",
        "my_df['target'] = my_df['target'].map({0: 0, 4: 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EFukCy-b6nL9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_df.target.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JYb299ov7Dvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Visualisation"
      ]
    },
    {
      "metadata": {
        "id": "HtE5CPs87JBM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to implement a couple of data visualisation in the next step, we need term frequency data. What kind of words are used in the tweets, and how many times it is used in entire corpus. We use count vectorizer to calculate the term frequencies, even though the count vectorizer is also for fit, train and predict, but at this stage, we will just be extracting the term frequencies for the visualisation.\n",
        "\n",
        "There are parameter options available for count vectorizer, such as removing stop words, limiting the maximum number of terms. However, in order to get a full picture of the dataset first, we implemente with stop words included, and not limiting the maximum number of terms."
      ]
    },
    {
      "metadata": {
        "id": "Nt4rC63X61i0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cvec = CountVectorizer()\n",
        "cvec.fit(my_df.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQEPgqa07WzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(cvec.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tD6SBh5w7mPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the count vectorizer has extracted 264,936 words out of the corpus."
      ]
    },
    {
      "metadata": {
        "id": "DxtilNWx7xMq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "tIaC65kg7jnV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Get the term frequency in a sparce matric\n",
        "neg_doc_matrix = cvec.transform(my_df[my_df.target == 0].text)\n",
        "pos_doc_matrix = cvec.transform(my_df[my_df.target == 1].text)\n",
        "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
        "pos_tf = np.sum(pos_doc_matrix,axis=0)\n",
        "neg = np.squeeze(np.asarray(neg_tf))\n",
        "pos = np.squeeze(np.asarray(pos_tf))\n",
        "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lx_MLT_kA7Fc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "term_freq_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IovLNnVwBO99",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "term_freq_df.columns = ['negative', 'positive']\n",
        "term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive']\n",
        "term_freq_df.sort_values(by='total', ascending=False).iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PAbX3VRBrFc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Zipf’s Law"
      ]
    },
    {
      "metadata": {
        "id": "x68-51WxCESu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Zipf's Law is first presented by French stenographer Jean-Baptiste Estoup and later named after the American linguist George Kingsley Zipf. Zipf's Law states that a small number of words are used all the time, while the vast majority are used very rarely. There is nothing surprising about this, we know that we use some of the words very frequently, such as \"the\", \"of\", etc, and we rarely use the words like \"aardvark\" (aardvark is an animal species native to Africa). However, what's interesting is that \"given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\"\n",
        "\n",
        "In other words, the rth most frequent word has a frequency f(r) that scales according to\n",
        "\n",
        "$${f(r)} \\propto \\frac{1}{r^\\alpha}$$ for $$\\alpha \\approx {1}$$\n",
        "\n",
        "Let's see how the tweet tokens and their frequencies look like on a plot."
      ]
    },
    {
      "metadata": {
        "id": "yPrX0sp2BsDk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pos = np.arange(500)\n",
        "plt.figure(figsize=(10,8))\n",
        "s = 1\n",
        "expected_zipf = [term_freq_df.sort_values(by='total', ascending=False)['total'][0]/(i+1)**s for i in y_pos]\n",
        "plt.bar(y_pos, term_freq_df.sort_values(by='total', ascending=False)['total'][:500], align='center', alpha=0.5)\n",
        "plt.plot(y_pos, expected_zipf, color='r', linestyle='--',linewidth=2,alpha=0.5)\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 500 tokens in tweets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GVPPLD6tCJaL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On the X-axis is the rank of the frequency from highest rank from left up to 500th rank to the right. Y-axis is the frequency observed in the corpus (in this case, “Sentiment140” dataset). One thing to note is that the actual observations in most cases does not strictly follow Zipf’s distribution, but rather follow a trend of “near-Zipfian” distribution.\n",
        "\n",
        "Even though we can see the plot follows the trend of Zipf’s Law, but it looks like it has more area above the expected Zipf curve in higher ranked words.\n",
        "\n",
        "Another way to plot this is on a log-log graph, with X-axis being log(rank), Y-axis being log(frequency). By plotting on a log-log scale the result will yield roughly linear line on the graph."
      ]
    },
    {
      "metadata": {
        "id": "_jFlbqhxCGaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pylab import *\n",
        "counts = term_freq_df.total\n",
        "tokens = term_freq_df.index\n",
        "ranks = arange(1, len(counts)+1)\n",
        "indices = argsort(-counts)\n",
        "frequencies = counts[indices]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.ylim(1,10**6)\n",
        "plt.xlim(1,10**6)\n",
        "loglog(ranks, frequencies, marker=\".\")\n",
        "plt.plot([1,frequencies[0]],[frequencies[0],1],color='r')\n",
        "title(\"Zipf plot for tweets tokens\")\n",
        "xlabel(\"Frequency rank of token\")\n",
        "ylabel(\"Absolute frequency of token\")\n",
        "grid(True)\n",
        "for n in list(logspace(-0.5, log10(len(counts)-2), 25).astype(int)):\n",
        "    dummy = text(ranks[n], frequencies[n], \" \" + tokens[indices[n]], \n",
        "                 verticalalignment=\"bottom\",\n",
        "                 horizontalalignment=\"left\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "94P-5CKQCny8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokens Visualisation\n"
      ]
    },
    {
      "metadata": {
        "id": "AZ-7uAz1Cca7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#remove stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cvec = CountVectorizer(stop_words='english',max_features=10000)\n",
        "cvec.fit(my_df.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2nNK4HsFDUnZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "document_matrix = cvec.transform(my_df.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KILTdCcBDdbR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#term frequency in negative text\n",
        "%%time\n",
        "neg_batches = np.linspace(0,798179,10).astype(int)\n",
        "i=0\n",
        "neg_tf = []\n",
        "while i < len(neg_batches)-1:\n",
        "    batch_result = np.sum(document_matrix[neg_batches[i]:neg_batches[i+1]].toarray(),axis=0)\n",
        "    neg_tf.append(batch_result)\n",
        "    print (neg_batches[i+1],\"entries' term freuquency calculated\")\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fLZBACDDiBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#term frequency in positive text\n",
        "%%time\n",
        "pos_batches = np.linspace(798179,1596019,10).astype(int)\n",
        "i=0\n",
        "pos_tf = []\n",
        "while i < len(pos_batches)-1:\n",
        "    batch_result = np.sum(document_matrix[pos_batches[i]:pos_batches[i+1]].toarray(),axis=0)\n",
        "    pos_tf.append(batch_result)\n",
        "    print (pos_batches[i+1],\"entries' term freuquency calculated\")\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fs0YAQkhD-WR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#visualise frequency of term for positive and negative text\n",
        "neg = np.sum(neg_tf,axis=0)\n",
        "pos = np.sum(pos_tf,axis=0)\n",
        "term_freq_df2 = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()\n",
        "term_freq_df2.columns = ['negative', 'positive']\n",
        "term_freq_df2['total'] = term_freq_df2['negative'] + term_freq_df2['positive']\n",
        "term_freq_df2.sort_values(by='total', ascending=False).iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFICn094ELR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see what are the top 50 words in negative tweets on a bar chart."
      ]
    },
    {
      "metadata": {
        "id": "9meqn621EIDY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pos = np.arange(50)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.bar(y_pos, term_freq_df2.sort_values(by='negative', ascending=False)['negative'][:50], align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, term_freq_df2.sort_values(by='negative', ascending=False)['negative'][:50].index,rotation='vertical')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Top 50 negative tokens')\n",
        "plt.title('Top 50 tokens in negative tweets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7dxERPmmERRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's also take a look at top 50 positive tokens on a bar chart."
      ]
    },
    {
      "metadata": {
        "id": "UzaI5M4UENXy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pos = np.arange(50)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.bar(y_pos, term_freq_df2.sort_values(by='positive', ascending=False)['positive'][:50], align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, term_freq_df2.sort_values(by='positive', ascending=False)['positive'][:50].index,rotation='vertical')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Top 50 positive tokens')\n",
        "plt.title('Top 50 tokens in positive tweets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pFDUBjZBEX31",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "neutral words like \"just\", \"day\", are quite high up in the rank"
      ]
    },
    {
      "metadata": {
        "id": "CcBsaeSkEb5C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plot negative frequency of a word on X-axis, and positive frequency on Y-axis"
      ]
    },
    {
      "metadata": {
        "id": "rdQ0zbWREULM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(8,6))\n",
        "ax = sns.regplot(x=\"negative\", y=\"positive\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
        "plt.ylabel('Positive Frequency')\n",
        "plt.xlabel('Negative Frequency')\n",
        "plt.title('Negative Frequency vs Positive Frequency')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rKfgKZM3EfhQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPufkbokEzq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Most of the words are below 10,000 on both X-axis and Y-axis, and we cannot see a meaningful relation between negative and positive frequency.\n",
        "\n",
        "In order to come up with a meaningful metric which can characterise important tokens in each class, I borrowed a metric presented by Jason Kessler in PyData 2017 Seattle. In the talk, he presented a Python library called Scattertext. Even though I did not make use of the library, the metrics used in the Scattertext as a way of visualising text data are very useful in filtering meaningful tokens from the frequency data.\n",
        "\n",
        "Intuitively, if a word appears more often in one class compared to another, this can be a good measure of how much the word is meaningful to characterise the class. In the below code I named it as 'pos_rate', and as you can see from the calculation of the code, this is defined as\n",
        "\n",
        "pos_rate = positive frequency / positive frequency + negative frequency"
      ]
    },
    {
      "metadata": {
        "id": "Wm9z0DQiFB0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "term_freq_df2['pos_rate'] = term_freq_df2['positive'] * 1./term_freq_df2['total']\n",
        "term_freq_df2.sort_values(by='pos_rate', ascending=False).iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRaxQnkfFIkT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Words with highest pos_rate have zero frequency in the negative tweets, but overall frequency of these words are too low to think of it as a guideline for positive tweets."
      ]
    },
    {
      "metadata": {
        "id": "Q5mRrHYQFM83",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another metric is the frequency a words occurs in the class. This is defined as\n",
        "\n",
        "pos_freq_pct = positive frequency / SUM positive frequency"
      ]
    },
    {
      "metadata": {
        "id": "QABrJSC4FD0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "term_freq_df2['pos_freq_pct'] = term_freq_df2['positive'] * 1./term_freq_df2['positive'].sum()\n",
        "term_freq_df2.sort_values(by='pos_freq_pct', ascending=False).iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVPuG_Q8Fc5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But since pos_freq_pct is just the frequency scaled over the total sum of the frequency, the rank of pos_freq_pct is exactly same as just the positive frequency.\n",
        "\n",
        "What we can do now is to combine pos_rate, pos_freq_pct together to come up with a metric which reflects both pos_rate and pos_freq_pct. Even though both of these can take a value ranging from 0 to 1, pos_rate has much wider range actually spanning from 0 to 1, while all the pos_freq_pct values are squashed within the range smaller than 0.015. If we average these two numbers, pos_rate will be too dominant, and will not reflect both metrics effectively.\n",
        "\n",
        "So here we use harmonic mean instead of arithmetic mean. \"Since the harmonic mean of a list of numbers tends strongly toward the least elements of the list, it tends (compared to the arithmetic mean) to mitigate the impact of large outliers and aggravate the impact of small ones.\" The harmonic mean H of the positive real number x1,x2,...xn is defined as $${H} = \\frac {n}{\\sum_{i=1}^{n}\\ \\frac{1}{x_i}}$$"
      ]
    },
    {
      "metadata": {
        "id": "r7huQutwFW6P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import hmean\n",
        "\n",
        "term_freq_df2['pos_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['pos_rate'], x['pos_freq_pct']])\n",
        "                                                                   if x['pos_rate'] > 0 and x['pos_freq_pct'] > 0 \n",
        "                                                                   else 0), axis=1)                                                        \n",
        "term_freq_df2.sort_values(by='pos_hmean', ascending=False).iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y0BufMkGF5ou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The harmonic mean rank seems like the same as pos_freq_pct. By calculating the harmonic mean, the impact of small value (in this case, pos_freq_pct) is too aggravated and ended up dominating the mean value. This is again exactly same as just the frequency value rank and doesn't provide a meaningful result.\n",
        "\n",
        "What we can try next is to get the CDF (Cumulative Distribution Function) value of both pos_rate and pos_freq_pct. CDF can be explained as \"distribution function of X, evaluated at x, is the probability that X will take a value less than or equal to x\". By calculating CDF value, we can see where the value of either pos_rate or pos_freq_pct lies in the distribution in terms of cumulative manner. In the below result of the code, we can see a word \"welcome\" with pos_rate_normcdf of 0.995625, and pos_freq_pct_normcdf of 0.999354. This means roughly 99.56% of the tokens will take a pos_rate value less than or equal to 0.91535, and 99.99% will take a pos_freq_pct value less than or equal to 0.001521.\n",
        "\n",
        "Next, we calculate a harmonic mean of these two CDF values, as we did earlier. By calculating the harmonic mean, we can see that pos_normcdf_hmean metric provides a more meaningful measure of how important a word is within the class."
      ]
    },
    {
      "metadata": {
        "id": "1fGHhJahFrGn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "def normcdf(x):\n",
        "    return norm.cdf(x, x.mean(), x.std())\n",
        "\n",
        "term_freq_df2['pos_rate_normcdf'] = normcdf(term_freq_df2['pos_rate'])\n",
        "term_freq_df2['pos_freq_pct_normcdf'] = normcdf(term_freq_df2['pos_freq_pct'])\n",
        "term_freq_df2['pos_normcdf_hmean'] = hmean([term_freq_df2['pos_rate_normcdf'], term_freq_df2['pos_freq_pct_normcdf']])\n",
        "term_freq_df2.sort_values(by='pos_normcdf_hmean', ascending=False).iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fb2tFlNMGAe0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next step is to apply the same calculation to negative frequency of each word.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mafCe_yEF4x-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "term_freq_df2['neg_rate'] = term_freq_df2['negative'] * 1./term_freq_df2['total']\n",
        "term_freq_df2['neg_freq_pct'] = term_freq_df2['negative'] * 1./term_freq_df2['negative'].sum()\n",
        "term_freq_df2['neg_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['neg_rate'], x['neg_freq_pct']])\n",
        "                                                                   if x['neg_rate'] > 0 and x['neg_freq_pct'] > 0 \n",
        "                                                                   else 0), axis=1)                                                        \n",
        "term_freq_df2['neg_rate_normcdf'] = normcdf(term_freq_df2['neg_rate'])\n",
        "term_freq_df2['neg_freq_pct_normcdf'] = normcdf(term_freq_df2['neg_freq_pct'])\n",
        "term_freq_df2['neg_normcdf_hmean'] = hmean([term_freq_df2['neg_rate_normcdf'], term_freq_df2['neg_freq_pct_normcdf']])\n",
        "term_freq_df2.sort_values(by='neg_normcdf_hmean', ascending=False).iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XVk8D5IOGNOU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's see how the values are converted on a plot. In order to compare, I will first plot neg_hmean vs pos_hmean, and neg_normcdf_hmean vs pos_normcdf_hmean."
      ]
    },
    {
      "metadata": {
        "id": "EpKLtL40GC7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "ax = sns.regplot(x=\"neg_hmean\", y=\"pos_hmean\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
        "plt.ylabel('Positive Rate and Frequency Harmonic Mean')\n",
        "plt.xlabel('Negative Rate and Frequency Harmonic Mean')\n",
        "plt.title('neg_hmean vs pos_hmean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3s2_GpMkGVqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Not much difference from the just frequency of positive and negative. How about the CDF harmonic mean?\n"
      ]
    },
    {
      "metadata": {
        "id": "2YmtJ2TJGN_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "ax = sns.regplot(x=\"neg_normcdf_hmean\", y=\"pos_normcdf_hmean\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\n",
        "plt.ylabel('Positive Rate and Frequency CDF Harmonic Mean')\n",
        "plt.xlabel('Negative Rate and Frequency CDF Harmonic Mean')\n",
        "plt.title('neg_normcdf_hmean vs pos_normcdf_hmean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SlKOQBv9GfFs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It seems like the harmonic mean of rate CDF and frequency CDF has created an interesting pattern on the plot. If a data point is near to upper left corner, it is more positive, and if it is closer to the bottom right corner, it is more negative.\n",
        "\n",
        "It is good that the metric has created some meaningful insight out of frequency, but with text data, showing every token as just a dot is lacking important information on which token each data point represents. With 10,000 points, it is difficult to annotate all of the points on the plot. For this part, I have tried several methods and came to a conclusion that it is not very practical or feasible to directly annotate data points on the plot.\n",
        "\n",
        "So I took an alternative method of the interactive plot with Bokeh. Bokeh is an interactive visualisation library for Python, which creates graphics in style of D3.js. Bokeh can output the result in HTML format or also within the Jupyter Notebook. And below is the plot created with Bokeh."
      ]
    },
    {
      "metadata": {
        "id": "6TUU66Z2GZ7H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bokeh.plotting import figure\n",
        "from bokeh.io import output_notebook, show\n",
        "from bokeh.models import LinearColorMapper\n",
        "output_notebook()\n",
        "color_mapper = LinearColorMapper(palette='Inferno256', low=min(term_freq_df2.pos_normcdf_hmean), high=max(term_freq_df2.pos_normcdf_hmean))\n",
        "p = figure(x_axis_label='neg_normcdf_hmean', y_axis_label='pos_normcdf_hmean')\n",
        "p.circle('neg_normcdf_hmean','pos_normcdf_hmean',size=5,alpha=0.3,source=term_freq_df2,color={'field': 'pos_normcdf_hmean', 'transform': color_mapper})\n",
        "from bokeh.models import HoverTool\n",
        "hover = HoverTool(tooltips=[('token','@index')])\n",
        "p.add_tools(hover)\n",
        "show(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rP_ol0qfLWSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ]
    },
    {
      "metadata": {
        "id": "ebxjgNWELbnN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we can train any model, we first consider how to split the data. Here I chose to split the data into three chunks: train, development, test. I referenced Andrew Ng's \"deeplearning.ai\" course on how to split the data.\n",
        "\n",
        "**Train set:** The sample of data used for learning\n",
        "\n",
        "**Development set** (Hold-out cross-validation set): The sample of data used to tune the parameters of a classifier, and provide an unbiased evaluation of a model.\n",
        "\n",
        "**Test set:** The sample of data used only to assess the performance of a final model.\n",
        "\n",
        "The ratio I decided to split my data is 98/1/1, 98% of data as the training set, and 1% for the dev set, and the final 1% for the test set. The rationale behind this ratio comes from the size of my whole data set. The dataset has more than 1.5 million entries. In this case, only 1% of the whole data gives me more than 15,000 entries. This is more than enough to evaluate the model and refine the parameters.\n",
        "\n",
        "Another approach is splitting the data into only train and test set, and run k-fold cross-validation on the training set, so that you can have an unbiased evaluation of a model. But considering the size of the data, I have decided to use the train set only to train a model, and evaluate on the dev set, so that I can quickly test different algorithms and run this process iteratively."
      ]
    },
    {
      "metadata": {
        "id": "5avvxrhBLX45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = my_df.text\n",
        "y = my_df.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q5scJEYjLzvo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "SEED = 2000\n",
        "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
        "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bEVKPwSYL1No",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print (\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
        "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
        "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
        "print (\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
        "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
        "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
        "print (\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
        "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
        "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7kEAWI6oMNwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "\n",
        "When comparing various machine learning algorithms, baseline provides a point of reference to compare. The most popular baseline is the Zero Rule (ZeroR). ZeroR classifier simply predicts the majority category (class). Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods. As you can see from the above validation set class division, the majority class is negative with 50.40%, which means if a classifier predicts negative for every validation data, it will get 50.40% accuracy.\n",
        "\n",
        "Another baseline I wanted to compare the validation results with is TextBlob. Textblob is a python library for processing textual data. Apart from other useful tools such as POS tagging, n-gram, The package has built-in sentiment classification. This is a so-called out-of-the-box sentiment analysis tool, and in addition to the null accuracy, I will also keep in mind of the accuracy I get from TextBlob sentiment analysis to see how my model is performing."
      ]
    },
    {
      "metadata": {
        "id": "zLQwWd9AL_Dp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khZJRoUoMZmn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tbresult = [TextBlob(i).sentiment.polarity for i in x_validation]\n",
        "tbpred = [0 if n < 0 else 1 for n in tbresult]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V85AHoJBMbJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conmat = np.array(confusion_matrix(y_validation, tbpred, labels=[1,0]))\n",
        "\n",
        "confusion = pd.DataFrame(conmat, index=['positive', 'negative'],\n",
        "                         columns=['predicted_positive','predicted_negative'])\n",
        "print (\"Accuracy Score: {0:.2f}%\".format(accuracy_score(y_validation, tbpred)*100))\n",
        "print (\"-\"*80)\n",
        "print (\"Confusion Matrix\\n\")\n",
        "print (confusion)\n",
        "print (\"-\"*80)\n",
        "print (\"Classification Report\\n\")\n",
        "print (classification_report(y_validation, tbpred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AvgJgkuQMr0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TextBlob sentiment analysis yielded 61.84% accuracy on the validation set, which is 10.25% more accurate than null accuracy (50.40%)"
      ]
    },
    {
      "metadata": {
        "id": "qFhWsuD6M8Ne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature extraction\n",
        "\n",
        "If we want to use text in machine learning algorithms, we’ll have to convert them to a numerical representation. One of the methods is called bag-of-words approach. The bag of words model ignores grammar and order of words. Once we have a corpus (text data) then first, a list of vocabulary is created based on the entire corpus. Then each document or data entry is represented as numerical vectors based on the vocabulary built from the corpus."
      ]
    },
    {
      "metadata": {
        "id": "QLftDsqgNIkr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Count Vectorizer\n",
        "\n",
        "With count vectorizer, we merely count the appearance of the words in each text. For example, let's say we have 3 documents in a corpus: \"I love dogs\", \"I hate dogs and knitting\", \"Knitting is my hobby and my passion\". If we build vocabulary from these three sentences and represent each document as count vectors, it will look like below pictures.\n",
        "\n",
        "![alt text](https://github.com/tthustla/twitter_sentiment_analysis_part4/raw/83468d32775781d23ea4e6ab8bd4178c93178f38/img/cvec.jpeg)\n",
        "\n",
        "\n",
        "But if the size of the corpus gets big, the number of vocabulary gets too big to process. With my 1.5 million tweets, if I build vocabulary without limiting the number of vocabulary, I will have more than 260,000 vocabularies. This means that the shape of training data will be around 1,500,000 x 260,000, this sounds too big to train various different models with. So I decided to limit the number of vocabularies, but I also wanted to see how the performance varies depending on the number of vocabularies.\n",
        "\n",
        "Another thing I wanted to explore is stopwords. Stop Words are words which do not contain important significance, such as \"the\", \"of\", etc. It is often assumed that removing stopwords is a necessary step, and will improve the model performance. But I wanted to see for myself if this is really the case. So I ran the same test with and without stop words and compared the result. In addition, I also defined my custom stopwords list, which contains top 10 most frequent words in the corpus: \"to\", \"the\", \"my\", \"it\", \"and\", \"you\", \"not\", \"is\", \"in\", \"for\".\n",
        "\n",
        "A model I chose to evaluate different count vectors is the logistic regression. It is one of the linear models, so computationally scalable to big data, compared to models like KNN or random forest. And once I have the optimal number of features and make a decision on whether to remove stop words or not, then I will try different models with the chosen number of vocabularies' count vectors."
      ]
    },
    {
      "metadata": {
        "id": "gUMGW0lqMeRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oghOkZFUNtxQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below I define two functions to iteratively train on a different number of features, then check the accuracy of logistic regression on the validation set.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-lRtYvlFNqvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
        "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
        "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
        "    else:\n",
        "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
        "    t0 = time()\n",
        "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
        "    y_pred = sentiment_fit.predict(x_test)\n",
        "    train_test_time = time() - t0\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
        "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
        "    if accuracy > null_accuracy:\n",
        "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
        "    elif accuracy == null_accuracy:\n",
        "        print (\"model has the same accuracy with the null accuracy\")\n",
        "    else:\n",
        "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
        "    print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
        "    print (\"-\"*80)\n",
        "    return accuracy, train_test_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RAokIX4eNv4X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cvec = CountVectorizer()\n",
        "lr = LogisticRegression()\n",
        "n_features = np.arange(10000,100001,10000)\n",
        "\n",
        "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
        "    result = []\n",
        "    print (classifier)\n",
        "    print (\"\\n\")\n",
        "    for n in n_features:\n",
        "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
        "        checker_pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', classifier)\n",
        "        ])\n",
        "        print (\"Validation result for {} features\".format(n))\n",
        "        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
        "        result.append((n,nfeature_accuracy,tt_time))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWX962FVOO-m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Unigram"
      ]
    },
    {
      "metadata": {
        "id": "MyKzwMcgOC3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print (\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\n",
        "feature_result_wosw = nfeature_accuracy_checker(stop_words='english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f8BWYLvKOR4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print (\"RESULT FOR UNIGRAM WITH STOP WORDS\\n\")\n",
        "feature_result_ug = nfeature_accuracy_checker()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPI_TiGeAvEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
        "#nfeatures_plot_ug_wocsw = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
        "nfeatures_plot_ug_wosw = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='with stop words')\n",
        "#plt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy,label='without custom stop words')\n",
        "plt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy,label='without stop words')\n",
        "plt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\n",
        "plt.xlabel(\"Number of features\")\n",
        "plt.ylabel(\"Validation set accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVQZMU6_BmFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train linear regression model with bigrams\n",
        "%%time\n",
        "print (\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\n",
        "feature_result_bg = nfeature_accuracy_checker(ngram_range=(1, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RDlx6jv7HDyY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "feature_result_wosw_file = 'feature_result_wosw.sav'\n",
        "pickle.dump(feature_result_wosw, open(feature_result_wosw_file, 'wb'))\n",
        "\n",
        "feature_result_ug_file = 'feature_result_ug.sav'\n",
        "pickle.dump(feature_result_ug, open(feature_result_ug_file, 'wb'))\n",
        "\n",
        "feature_result_bg_file = 'feature_result_bg.sav'\n",
        "pickle.dump(feature_result_bg, open(feature_result_bg_file, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FeHzI4t2BxST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train linear regression model with trigrams\n",
        "%%time\n",
        "print (\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\n",
        "feature_result_tg = nfeature_accuracy_checker(ngram_range=(1, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mH3UB-LZB8Bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nfeatures_plot_tg = pd.DataFrame(feature_result_tg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
        "nfeatures_plot_bg = pd.DataFrame(feature_result_bg,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
        "nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram')\n",
        "plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram')\n",
        "plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram')\n",
        "plt.title(\"N-gram(1~3) test result : Accuracy\")\n",
        "plt.xlabel(\"Number of features\")\n",
        "plt.ylabel(\"Validation set accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5eMth_KVFI_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
        "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
        "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
        "    else:\n",
        "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
        "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
        "    y_pred = sentiment_fit.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
        "    confusion = pd.DataFrame(conmat, index=['negative', 'positive'],\n",
        "                         columns=['predicted_negative','predicted_positive'])\n",
        "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
        "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
        "    if accuracy > null_accuracy:\n",
        "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
        "    elif accuracy == null_accuracy:\n",
        "        print (\"model has the same accuracy with the null accuracy\")\n",
        "    else:\n",
        "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
        "    print (\"-\"*80)\n",
        "    print (\"Confusion Matrix\\n\")\n",
        "    print (confusion)\n",
        "    print (\"-\"*80)\n",
        "    print (\"Classification Report\\n\")\n",
        "    print (classification_report(y_test, y_pred, target_names=['negative','positive']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_uOQjNJFMHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ug_cvec = CountVectorizer(max_features=90000)\n",
        "ug_pipeline = Pipeline([\n",
        "        ('vectorizer', ug_cvec),\n",
        "        ('classifier', lr)\n",
        "    ])\n",
        "train_test_and_evaluate(ug_pipeline, x_train, y_train, x_validation, y_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eqo_XyhGFNwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "bg_cvec = CountVectorizer(max_features=70000,ngram_range=(1, 2))\n",
        "bg_pipeline = Pipeline([\n",
        "        ('vectorizer', bg_cvec),\n",
        "        ('classifier', lr)\n",
        "    ])\n",
        "train_test_and_evaluate(bg_pipeline, x_train, y_train, x_validation, y_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dqux__9KFPT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tg_cvec = CountVectorizer(max_features=80000,ngram_range=(1, 3))\n",
        "tg_pipeline = Pipeline([\n",
        "        ('vectorizer', tg_cvec),\n",
        "        ('classifier', lr)\n",
        "    ])\n",
        "train_test_and_evaluate(tg_pipeline, x_train, y_train, x_validation, y_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoUT3TUdGKvU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}